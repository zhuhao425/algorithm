# 多雷达应用方案

## 背景

&nbsp;&nbsp;&nbsp;&nbsp;在移动机器人的应用中，由于传感器都会存在一定的局限性，所以单传感器的使用往往不能满足应用的需求。一个移动机器人身上往往会装备多个雷达，IMU，相机等各种传感器。各传感器之间的可以互相弥补对方的不足，尽可能的去满足应用的要求。

&nbsp;&nbsp;&nbsp;&nbsp;多雷达的应用是多传感器应用中比较简单的一个部分。因为雷达所提供的数据间不存在明显差异，不像雷达和相机融合时，雷达提供的是点云，而相机提供的是图像，两者的数据之间差异明显。多雷达的使用的目的是增加扫描范围和扫描密度，这样可以缓解单雷达使用时出现的漏检问题。

## 应用

&nbsp;&nbsp;&nbsp;&nbsp;多雷达的应用方案中会存在以下三个问题：雷达的放置、数据校准以及数据融合算法。

### 雷达的放置

&nbsp;&nbsp;多雷达的应用首先需要考虑的就是如何放置这些雷达。我们在考虑使用多雷达的时候通常都是为了克服在应用单雷达的时候会出现的一系列问题。而多个雷达的放置位置能够直接影响到多雷达在应用时的性能。但是雷达的放置还需要考虑到地盘。所以目前没有特别好的放置方法，只能说根据应用的要求，尽可能的选择好的放置方法。

### 数据校准

&nbsp;&nbsp;&nbsp;&nbsp;雷达的数据的时间戳都是使用的内部时钟。这样，不同的雷达的时间就会出现时间戳不一致的问题。因此，在使用多个雷达的时候，我们首先要考虑的就是时间同步的问题。一般来讲，时间同步可以分为硬同步和软同步。硬同步是在硬件的层面上去同步，也就是说会去改造或者增加硬件来达到同步的效果。软同步则是使用软件来进行时间同步，ROS也提供的相应的API[1]。

&nbsp;&nbsp;&nbsp;&nbsp;时间同步完成后还面临另外一个问题：各雷达的数据的坐标系都是以自身为原点。这样，不同的雷达的坐标系会不一样，需要将它们转换到同一个坐标系下。坐标系的转换涉及到一系列转换参数，如旋转矩阵、欧拉角、四元数等变换方法中对应的参数。理论上讲，按照雷达的放置位置可以手动的完成这些参数的标定。但是手动的标定难度太大，因此我们还是考虑使用自标定的方式。[2]中提供了一个单雷达的自标定程序，我们可以拿来对每个雷达分别进行标定。同时，论文[3]中还介绍了多个雷达的参数标定方法，可以一次完成多个雷达的参数标定。

### 数据融合算法

&nbsp;&nbsp;&nbsp;&nbsp;数据融合的方案可以按融合的时刻分为数据级融合、特征级融合以及决策级融合。对应的也就存在不同类型的算法。

#### 数据级融合

&nbsp;&nbsp;&nbsp;&nbsp;数据级的融合即在数据获取的阶段就进行融合。当处理模块将得到的激光雷达数据进行校准后，融合模块可以直接将多个雷达的点云放置到一个点云下。后续的定位、建图等模块就直接将该点云数据看作是一个雷达提供的进行处理。可见，该方案的特点就是简单，容易实现。但是将多个雷达的数据直接融合成一个点云会大大增加点云的数据量，导致后续应用模块的计算量增加。对应demo代码可以参见[4]。


#### 特征级融合

&nbsp;&nbsp;&nbsp;&nbsp;一个雷达所提供的点云数据中的会存在很多无用的信息。因此我们也可以考虑使用特征级融合。也就是在处理模块将激光雷达的数据进行校准后，算法会对雷达的数据进行预处理， 提取出特征后再进行融合。如[5]中，在acml的观测更新部分，两个雷达分别进行扫描匹配，然后共同更新粒子权重。


#### 决策级融合

&nbsp;&nbsp;&nbsp;&nbsp;决策级的融合则是将各雷达的处理结果进行融合。常见的算法有贝叶斯推理、D-S表决算法以及卡尔曼滤波。其中卡尔曼滤波中，算法对多个雷达的数据分开进行定位。这样，每一个雷达就可以得到一个带有噪声，且噪声服从高斯分布的定位结果。高斯分布的参数，均值和方差代表则该结果的理想程度。当算法得到多个定位结果时，它会根据结果的均值和方差进行计算，得到一个更加理想的定位结果。EKF_SLAM代码见[6]。

## 总结

&nbsp;&nbsp;&nbsp;&nbsp;虽然方案有多种，不过我们暂时不去选择比较难实现的方案。所以目前可以暂时考虑在机器人上同一水平线上放置多个雷达，且这多个雷达以“前后”的方式放置。对应的算法方案有：

1.自标定+数据级融合+SLAM算法(如gmapping)

2.自标定+acml特征融合

3.自标定+各雷达使用ekf_slam+将服从高斯分布的结果进行融合

## 参考文献

1.http://wiki.ros.org/message_filters

2.https://github.com/I-am-Unique/lidar_align_tool

3.Sheehan M , Harrison A , Newman P . Automatic Self-calibration of a Full Field-of-View 3D n-Laser Scanner[J]. Springer Tracts in Advanced Robotics.

4.https://github.com/I-am-Unique/lidar_perception

5.https://github.com/adayimaxiga/double_laser_mapping_location

6.http://wiki.ros.org/pose_ekf_slam